{
    "dataset_reader": {
        "type": "syntactic_dependency_arc_prediction",
        "contextualizer": {
            "type": "precomputed_contextualizer",
            "representations_path": "contextualizers/elmo_original/ptb_syntactic_dependency.hdf5"
        },
        "negative_sampling_method": "balanced",
        # This saves memory and speeds up the model if we don't need access to the tokens in the model.
        "include_raw_tokens": false
    },
    "validation_dataset_reader": {
        "type": "syntactic_dependency_arc_prediction",
        "contextualizer": {
            "type": "precomputed_contextualizer",
            "representations_path": "contextualizers/elmo_original/ptb_syntactic_dependency.hdf5"
        },
        "negative_sampling_method": "balanced",
        # This saves memory and speeds up the model if we don't need access to the tokens in the model.
        "include_raw_tokens": false
    },
    "train_data_path": "data/syntactic_dependency/ptb.train.conllu",
    "validation_data_path": "data/syntactic_dependency/ptb.dev.conllu",
    "test_data_path": "data/syntactic_dependency/ptb.test.conllu",
    "evaluate_on_test" : true,
    "model": {
        "type": "pairwise_tagger",
        "decoder": "mlp",
        "encoder": {
            "type": "lstm",
            "input_size": 1024,
            "hidden_size": 512,
            "bidirectional": true,
            "num_layers": 2
        },
        "token_representation_dim": 1024,
        "combination": "x,y,x*y"
    },
    "iterator": {
        "type": "basic",
        "batch_size" : 80
    },
    "trainer": {
        "num_epochs": 50,
        "patience": 3,
        "cuda_device": 0,
        "validation_metric": "+accuracy",
        "optimizer": {
            "type": "adam",
            "lr": 0.001
        }
    }
}
